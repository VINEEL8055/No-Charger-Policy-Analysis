# -*- coding: utf-8 -*-
"""Charger-Policy-Analysis-Part-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VZxU7LgJyt5lHiDqGrWaXeznpSnin1bx
"""

import requests
import pandas as pd

import requests
import pandas as pd

# API Keys (Replace with your own keys)
NEWSAPI_KEY = "3b983d060c2240ccaedcacdcdc7f29af"
MEDIASTACK_KEY = "d630631db8ad34e3ef4bc452b92009e8"

# List of Expanded Queries
QUERIES = [
    "smartphone charger removal",
    "no charger with phone",
    "Apple no charger policy",
    "Samsung charger removal",
    "e-waste and smartphone chargers",
    "sustainable smartphone accessories"
]
PAGE_SIZE = 50  # Fetch 50 results per page (can be adjusted)

def fetch_newsapi(query):
    articles = []
    for page in range(1, 3):  # Fetch up to 2 pages (Adjust as needed)
        url = f"https://newsapi.org/v2/everything?q={query}&language=en&pageSize={PAGE_SIZE}&page={page}&apiKey={NEWSAPI_KEY}"
        response = requests.get(url).json()
        if "articles" in response:
            for article in response["articles"]:
                articles.append([
                    article.get("title", "No Title"),
                    article.get("description", "No Description"),
                    article["source"]["name"] if "source" in article else "Unknown",
                    article.get("publishedAt", "No Date"),
                    article.get("url", "No URL")
                ])
    return articles

def fetch_mediastack(query):
    url = f"http://api.mediastack.com/v1/news?access_key={MEDIASTACK_KEY}&languages=en&keywords={query}&limit=50"
    response = requests.get(url).json()
    articles = []
    if "data" in response:
        for article in response["data"]:
            articles.append([
                article.get("title", "No Title"),
                article.get("description", "No Description"),
                article.get("source", "Unknown"),
                article.get("published_at", "No Date"),
                article.get("url", "No URL")
            ])
    return articles

# Collecting articles from both APIs
all_articles = []
for query in QUERIES:
    all_articles.extend(fetch_newsapi(query))
    all_articles.extend(fetch_mediastack(query))

# Creating DataFrame
df_combined = pd.DataFrame(all_articles, columns=["title", "description", "source", "published_date", "url"]).drop_duplicates()

# Save to CSV
df_combined.to_csv("smartphone_charger_news.csv", index=False)

# Show Summary
print(f"Total Articles Collected: {len(df_combined)}")
print(df_combined.head())

df_combined.info()

!pip install feedparser

import feedparser
import pandas as pd

# Define multiple RSS feed URLs with different queries
QUERIES = [
    "smartphone charger removal",
    "no charger with phone",
    "Apple no charger policy",
    "Samsung charger removal"
]

BASE_URL = "https://news.google.com/rss/search?q={}&hl=en-GB&gl=GB&ceid=GB:en"

# Collect articles from all queries
news_list = []
for query in QUERIES:
    RSS_FEED_URL = BASE_URL.format(query.replace(" ", "+"))
    feed = feedparser.parse(RSS_FEED_URL)

    for entry in feed.entries:
        title = entry.title
        link = entry.link
        published = entry.published
        news_list.append([title, published, link, query])  # Add query for reference

# Convert to DataFrame
df_google_news = pd.DataFrame(news_list, columns=["Title", "Published Date", "URL", "Query"])

# Save to CSV
df_google_news.to_csv("google_news_expanded.csv", index=False)

# Print sample data
print(df_google_news.head())

import pandas as pd

# Load the datasets
file_google_news = "google_news_expanded.csv"
file_smartphone_news = "smartphone_charger_news.csv"

df_google_news = pd.read_csv(file_google_news)
df_smartphone_news = pd.read_csv(file_smartphone_news)

# Strip spaces from column names to ensure consistency
df_google_news.columns = df_google_news.columns.str.strip()
df_smartphone_news.columns = df_smartphone_news.columns.str.strip()

# Print available columns before selecting
print("Google News Columns:", df_google_news.columns.tolist())
print("Smartphone News Columns:", df_smartphone_news.columns.tolist())

# Select only relevant columns for merging
columns = ["Title", "Source", "Published Date", "URL"]

# Filter only available columns to avoid KeyError
df_google_news = df_google_news[[col for col in columns if col in df_google_news.columns]]
df_smartphone_news = df_smartphone_news[[col for col in columns if col in df_smartphone_news.columns]]

# Merge datasets and remove duplicates
df_combined = pd.concat([df_google_news, df_smartphone_news], ignore_index=True).drop_duplicates()

# Save the merged dataset
df_combined.to_csv("merged_labeled_news_data.csv", index=False)

# Show dataset summary
print(f"✅ Merged dataset saved as 'merged_labeled_news_data.csv' with {len(df_combined)} articles.")
print(df_combined.head())

pip install vaderSentiment

import pandas as pd
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Load dataset
file_path = "/content/merged_labeled_news_data.csv"
df = pd.read_csv(file_path)

# Initialize VADER
analyzer = SentimentIntensityAnalyzer()

# Function to analyze sentiment
def get_vader_sentiment(text):
    sentiment_score = analyzer.polarity_scores(str(text))["compound"]  # Get overall sentiment score

    if sentiment_score >= 0.05:
        return "Pro"  # Positive sentiment
    elif sentiment_score <= -0.05:
        return "Against"  # Negative sentiment
    else:
        return "Neutral"  # Neutral sentiment

# Apply VADER sentiment analysis
df["Label"] = df["Title"].apply(get_vader_sentiment)

# Save the improved labeled dataset
df.to_csv("vader_news_data.csv", index=False)

# Show summary
print(f"✅ Improved sentiment dataset saved as 'better_labeled_news_data.csv' with {len(df)} articles.")
print(df["Label"].value_counts())
print(df.head())

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the labeled dataset
file_path = "vader_news_data.csv"
df = pd.read_csv(file_path)

# Ensure "Label" column exists
if "Label" in df.columns:
    # Plot Sentiment Distribution
    plt.figure(figsize=(8,5))
    sns.countplot(x="Label", data=df, palette="coolwarm")
    plt.title("Sentiment Distribution in News Articles")
    plt.xlabel("Sentiment Category")
    plt.ylabel("Number of Articles")
    plt.show()
else:
    print("⚠️ 'Label' column not found in the dataset. Please check the data.")

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')
nltk.download('stopwords')

import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Load the dataset
file_path = "vader_news_data.csv"
df = pd.read_csv(file_path)

# Initialize preprocessing tools
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Function to clean text
def clean_text(text):
    text = re.sub(r'\W', ' ', str(text))  # Remove special characters
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = text.lower().strip()  # Convert to lowercase
    return text

# Function for Stemming
def apply_stemming(text):
    words = word_tokenize(clean_text(text))  # Tokenize & clean text
    words = [stemmer.stem(word) for word in words if word not in stop_words]
    return " ".join(words)

# Function for Lemmatization
def apply_lemmatization(text):
    words = word_tokenize(clean_text(text))  # Tokenize & clean text
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return " ".join(words)

# Apply Stemming & Lemmatization
df["Stemmed_Title"] = df["Title"].apply(apply_stemming)
df["Lemmatized_Title"] = df["Title"].apply(apply_lemmatization)
df[["Title", "Stemmed_Title", "Label"]].to_csv("stemmed_news_data.csv", index=False)
df[["Title", "Lemmatized_Title", "Label"]].to_csv("lemmatized_news_data.csv", index=False)

# Save cleaned dataset
df.to_csv("preprocessed_news_data.csv", index=False)

# Initialize Vectorizers
count_vectorizer = CountVectorizer(max_df=0.9, min_df=5, max_features=5000)
tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=5, max_features=5000)

# Apply Vectorization on Lemmatized Titles
count_matrix = count_vectorizer.fit_transform(df["Lemmatized_Title"])
tfidf_matrix = tfidf_vectorizer.fit_transform(df["Lemmatized_Title"])

# Convert to DataFrames
df_count = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Save vectorized datasets
df_count.to_csv("count_vectorized_news.csv", index=False)
df_tfidf.to_csv("tfidf_vectorized_news.csv", index=False)

# Print confirmation
print("✅ Preprocessing Complete! Datasets saved.")

