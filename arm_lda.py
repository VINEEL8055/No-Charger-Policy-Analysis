# -*- coding: utf-8 -*-
"""ARM_LDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BDmataxr2k3qmLzxENsgxp9Lsg7CELm4
"""

!pip install nltk
import nltk
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

import matplotlib.pyplot as plt
import networkx as nx

# Download required NLTK data
nltk.download('punkt')  # Download punkt tokenizer data
nltk.download('punkt_tab') # Download the punkt tokenizer models.
nltk.download('stopwords')

# ----------------------------------------
# Step 1: Load and Clean the Dataset
# ----------------------------------------
df = pd.read_csv("smartphone_charger_news.csv")
df['text'] = df['title'] + " " + df['description']

stop_words = set(stopwords.words('english'))
ps = PorterStemmer()

def preprocess(text):
    words = nltk.word_tokenize(text.lower())
    words = [ps.stem(w) for w in words if w.isalpha() and w not in stop_words and len(w) > 2]
    return list(set(words))  # remove duplicates within basket

transactions = df['text'].apply(preprocess).tolist()

# ----------------------------------------
# Step 2: One-Hot Encode the Transactions
# ----------------------------------------
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_encoded = pd.DataFrame(te_ary, columns=te.columns_)

# ----------------------------------------
# Step 3: Run Apriori Algorithm
# ----------------------------------------
frequent_itemsets = apriori(df_encoded, min_support=0.02, use_colnames=True, max_len=3)

# ----------------------------------------
# Step 4: Generate Rules
# ----------------------------------------
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

# Sort by support, confidence, lift
top_support = rules.sort_values(by="support", ascending=False).head(15)
top_confidence = rules.sort_values(by="confidence", ascending=False).head(15)
top_lift = rules.sort_values(by="lift", ascending=False).head(15)

print("Top 15 by Support:\n", top_support[['antecedents', 'consequents', 'support']])
print("\nTop 15 by Confidence:\n", top_confidence[['antecedents', 'consequents', 'confidence']])
print("\nTop 15 by Lift:\n", top_lift[['antecedents', 'consequents', 'lift']])

# ----------------------------------------
# Step 5: (Optional) Network Visualization of Lift Rules
# ----------------------------------------
def plot_network(rules_df, title):
    plt.figure(figsize=(10, 7))
    G = nx.DiGraph()

    for _, row in rules_df.iterrows():
        for a in row['antecedents']:
            for c in row['consequents']:
                G.add_edge(a, c, weight=row['lift'])

    pos = nx.spring_layout(G, k=0.5)
    nx.draw(G, pos, with_labels=True, node_color='skyblue', edge_color='gray', node_size=2500, font_size=10)
    plt.title(title)
    plt.show()

plot_network(top_lift, "Top Association Rules by Lift")
plot_network(top_support, "Top Association Rules by Support")
def plot_network(rules_df, title):
    plt.figure(figsize=(15, 12))
    G = nx.DiGraph()

    for _, row in rules_df.iterrows():
        for a in row['antecedents']:
            for c in row['consequents']:
                G.add_edge(a, c, weight=row['lift'])

    pos = nx.spring_layout(G, k=0.5)
    nx.draw(G, pos, with_labels=True, node_color='skyblue', edge_color='gray', node_size=2500, font_size=10)
    plt.title(title)
    plt.show()

plot_network(top_confidence, "Top Association Rules by Confidence")

"""# LDA"""

# -*- coding: utf-8 -*-
"""
News Topic Modeling using LDA (Adapted from Dr. Gates)
"""

import pandas as pd
import re
import string
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# -------------------------------------------
# STEP 1: Load News Data
# -------------------------------------------
df = pd.read_csv("smartphone_charger_news.csv")  # or use your NewsAPI output

# Combine title and description
df['text'] = df['title'].fillna('') + " " + df['description'].fillna('')
df = df[['published_date', 'source', 'title', 'text']].dropna()

# -------------------------------------------
# STEP 2: Clean the Headlines/Text
# -------------------------------------------
def clean_text(text):
    text = re.sub(r'[,.;@#?!&$\-\']+', ' ', text)
    text = re.sub(' +', ' ', text)
    text = re.sub(r'[^a-zA-Z]', " ", text)
    text = text.lower().strip()
    text = ' '.join([w for w in text.split() if len(w) > 3])
    return text

df['clean_text'] = df['text'].apply(clean_text)

# -------------------------------------------
# STEP 3: Vectorize
# -------------------------------------------
vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)
X = vectorizer.fit_transform(df['clean_text'])
vocab = vectorizer.get_feature_names_out()
vocab_array = np.asarray(vocab)
# -------------------------------------------
# STEP 4: LDA Modeling
# -------------------------------------------
num_topics = 5
lda_model = LatentDirichletAllocation(n_components=num_topics,
                                      max_iter=100,
                                      learning_method='online',
                                      random_state=42)
doc_topic_matrix = lda_model.fit_transform(X)

# -------------------------------------------
# STEP 5: Print Topics
# -------------------------------------------
def print_topics(model, vectorizer, top_n=10):
    for idx, topic in enumerate(model.components_):
        print(f"\nTopic #{idx}")
        print([(vectorizer.get_feature_names_out()[i], topic[i])
               for i in topic.argsort()[:-top_n - 1:-1]])

print_topics(lda_model, vectorizer, 15)

# -------------------------------------------
# STEP 6: Topic Word Distribution Visualization
# -------------------------------------------
word_topic = np.array(lda_model.components_).T
num_top_words = 15
fontsize_base = 16

plt.figure(figsize=(num_topics * 3, 6))
for t in range(num_topics):
    plt.subplot(1, num_topics, t + 1)
    plt.ylim(0, num_top_words + 1)
    plt.xticks([])
    plt.yticks([])
    plt.title(f"Topic #{t}")
    top_word_idx = word_topic[:, t].argsort()[::-1][:num_top_words]
    top_words = vocab_array[top_word_idx]
    for i, word in enumerate(top_words):
        plt.text(0.3, num_top_words - i, word, fontsize=fontsize_base)
plt.tight_layout()
plt.show()

from sklearn.decomposition import PCA

# 1. Top 30 terms for Topic 1 (index 1)
def plot_top_terms_for_topic(model, feature_names, topic_index=1, n_top_words=30):
    topic = model.components_[topic_index]
    top_indices = topic.argsort()[:-n_top_words - 1:-1]
    top_terms = [feature_names[i] for i in top_indices]
    top_weights = topic[top_indices]

    plt.figure(figsize=(10, 8))
    plt.barh(top_terms[::-1], top_weights[::-1], color='coral')
    plt.xlabel("Word Importance")
    plt.title(f"Top {n_top_words} Most Relevant Terms for Topic {topic_index}")
    plt.tight_layout()
    plt.show()

# 2. Inter-Topic Distance Map using PCA
def plot_inter_topic_distance_map(model):
    topic_vectors = model.components_
    pca = PCA(n_components=2)
    reduced = pca.fit_transform(topic_vectors)

    plt.figure(figsize=(8, 6))
    plt.scatter(reduced[:, 0], reduced[:, 1], color='skyblue', s=100)

    for i, (x, y) in enumerate(reduced):
        plt.text(x + 0.01, y + 0.01, f"Topic {i}", fontsize=12)

    plt.title("Inter-Topic Distance Map (PCA)")
    plt.xlabel("PCA Dimension 1")
    plt.ylabel("PCA Dimension 2")
    plt.tight_layout()
    plt.show()

# Generate both plots
plot_top_terms_for_topic(lda_model, feature_names, topic_index=1, n_top_words=30)
plot_inter_topic_distance_map(lda_model)

